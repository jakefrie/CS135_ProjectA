"""
SUMMARY:
- read in the files 
- extract the text columns from the CSVs 
- partition into training and validation - all subsequent steps are done with just training data
- count up all the words in all texts 
- create a bag of words based on our specification (exclude words appearing in <3 cods, or >90% of docs, for example)
- vectorize each text into the form of an array with length = [size of our bag of words]
- cross validation - partition data into equal parts, and keep one part as "Validation" while training on the rest
    we then average the score for each partition to come up with a total score for that set of hyperparameters
    - repeat for each set of hyperparameters
- Choose the best set of hyperparameters (based on AUROC)
- Finally, run this on the original validation set (that we havent used yet)
- Output the confusion matrix for this set
"""


Make Pipeline:
    Build the end-to-end modeling pipeline:
      text (strings)
        -> CountVectorizer (tokenize, prune vocab, create sparse count matrix)
        -> LogisticRegression (linear classifier on sparse counts)

Make RandomizerSearch
    """
    Configure a randomized hyperparameter search over both vectorizer and classifier.
    Cross-validation (StratifiedKFold) estimates each hyperparameter combo's performance.
    """
    A single call triggers:
      - vocabulary building (CountVectorizer.fit) on each CV train fold
      - vectorization (transform) for train/val folds
      - model fitting (LogisticRegression.fit) per fold
      - AUROC scoring per fold, averaged per hyperparameter combo
      
Create Model:
    Orchestrates:
      1. Extract text and labels
      2. Train/validation split (hold out 20% for final check)
      3. Hyperparameter search with CV on the training split
      4. Evaluate best model on held-out validation split
      5. Plot confusion matrix (threshold 0.5)